{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Crack_Segmentation_polygon_colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCVfTWGpSvLO"
      },
      "source": [
        "import sys, os\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import json\n",
        "from PIL import Image\n",
        "import glob\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQa1R686SvLU"
      },
      "source": [
        "# colab에서 사용한다면 \n",
        "colab = True"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIX4X159SvLU",
        "outputId": "47df1e34-97af-4036-ee93-edcba37eac86"
      },
      "source": [
        "if colab:\n",
        "  !git clone --depth 1 https://github.com/malheading/Surface_Crack_Segmentation.git"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'Surface_Crack_Segmentation' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFteHXPNSvLU"
      },
      "source": [
        "if colab:\n",
        "    img_positive_path = \"/content/Surface_Crack_Segmentation/Positive_jw/\"\n",
        "    img_negative_path = \"/content/Surface_Crack_Segmentation/Negative_jw/\"\n",
        "    label_positive_path = \"/content/Surface_Crack_Segmentation/Positive_Segmentation/\"\n",
        "    pass\n",
        "else:\n",
        "    img_positive_path = \"D:/_김정원/ss_class(AI)/Surface_Crack_Segmentation/Positive_jw/\"\n",
        "    img_negative_path = \"D:/_김정원/ss_class(AI)/Surface_Crack_Segmentation/Negative_jw/\"\n",
        "    label_positive_path = \"D:/_김정원/ss_class(AI)/Surface_Crack_Segmentation/Positive_Segmentation/\"\n",
        "    \n",
        "positive_imgs = sorted(glob.glob(img_positive_path + \"*.*\"))\n",
        "negative_imgs = sorted(glob.glob(img_negative_path + \"*.*\"))\n",
        "label_positive_datas = sorted(glob.glob(label_positive_path + \"*.*\"))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEGxcevenmaK"
      },
      "source": [
        "# 이미지 데이터들을 경로에서 불러옵니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfY7Xk6cSvLV",
        "outputId": "a32a2dbe-f9e4-407f-e7e4-fc97b4f70943"
      },
      "source": [
        "x_train_positive = []\n",
        "for i in range(len(positive_imgs)):\n",
        "    Im = Image.open(positive_imgs[i])\n",
        "    Im = Im.resize((128,128))\n",
        "    x_train_positive.append(np.array(Im))\n",
        "x_train_positive = np.array(x_train_positive,dtype=np.float32)\n",
        "\n",
        "x_train_negative = []\n",
        "for i in range(len(negative_imgs)):\n",
        "    Im = Image.open(negative_imgs[i])\n",
        "    Im = Im.resize((128,128))\n",
        "    x_train_negative.append(np.array(Im))\n",
        "x_train_negative = np.array(x_train_negative, dtype=np.float32)\n",
        "\n",
        "print(\"x_train_positive shape :\",x_train_positive.shape)\n",
        "print(\"x_train_negative shape :\",x_train_negative.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train_positive shape : (100, 128, 128, 3)\n",
            "x_train_negative shape : (100, 128, 128, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Sr1SOGGSvLW",
        "outputId": "14d861cc-2a71-43b4-8118-99d3ea50d64d"
      },
      "source": [
        "# 크랙이 있는 것과 없는 것 두가지를 concat해서 x_train 데이터를 만든다.\n",
        "x_train = np.concatenate((x_train_negative,x_train_positive))\n",
        "x_train = x_train/255.0\n",
        "print(\"x_train shape :\",x_train.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape : (200, 128, 128, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwrWqclOnsgA"
      },
      "source": [
        "# 균열이 있는 데이터의 라벨(영상 분할)데이터를 경로에서 불러옵니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RzZwiCISvLW"
      },
      "source": [
        "y_train_positive = []\n",
        "for i in range(len(label_positive_datas)):\n",
        "    Im = Image.open(label_positive_datas[i])\n",
        "    Im = Im.resize((128,128),Image.BOX)\n",
        "    y_train_positive.append(np.ceil(np.array(Im)))\n",
        "y_train_positive = np.array(y_train_positive,dtype=np.float32)/255.0\n",
        "y_train_positive = y_train_positive.reshape(y_train_positive.shape[0],y_train_positive.shape[1],y_train_positive.shape[2],1)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "feeqNsWYSvLW",
        "outputId": "1c4cb450-ca58-4612-ac1f-34fffb3e230c"
      },
      "source": [
        "# plt.imshow(y_train_positive[0])\n",
        "plt.imshow(y_train_positive[0].reshape((128,128)))\n",
        "# y_train_positive.dtype"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f98181cf0f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_Tw-ZvGn0eY"
      },
      "source": [
        "# 균열이 없는 데이터의 라벨은 모든 픽셀 값이 0이므로, numpy로 생성합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vg6zrs4vSvLX",
        "outputId": "b2ce9cd2-6554-4be8-93c9-f0d10fb9490a"
      },
      "source": [
        "# negative label은 모든 픽셀의 값이 0\n",
        "y_train_negative = np.zeros((x_train_negative.shape[0], 128, 128, 1),dtype=np.float32)\n",
        "\n",
        "# label(y_train)도 concat해준다\n",
        "y_train = np.concatenate((y_train_negative,y_train_positive))\n",
        "print(y_train.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(200, 128, 128, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pd9usyrySvLX"
      },
      "source": [
        "# 데이터셋 형태로 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81gXggeLSvLX"
      },
      "source": [
        "# from_tensor_slice로 데이터셋 생성\n",
        "BATCH_SIZE=10\n",
        "dataset = tf.data.Dataset.from_tensor_slices((x_train,y_train)).shuffle(10000).batch(BATCH_SIZE)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQ3JVQTdSvLY"
      },
      "source": [
        "# 트레인 데이터셋은 0.85 * 200 / 10 --> 170개\n",
        "# 테스트 데이터셋은 나머지 30개\n",
        "validation_split = 0.85\n",
        "train_dataset_size = int(y_train.shape[0] * validation_split / BATCH_SIZE)\n",
        "train_dataset = dataset.take(train_dataset_size)\n",
        "test_dataset = dataset.skip(train_dataset_size)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_hStCQpSvLY"
      },
      "source": [
        "# 모델 정의하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTzKRj45SvLY"
      },
      "source": [
        "CHANNEL = 3\n",
        "OUTPUT_CHANNELS = 3\n",
        "\n",
        "# 베이스모델로 MobileNetV2를 사용해서, 조금 더 가벼운 네트워크를 구성하고자 합니다.\n",
        "base_model = tf.keras.applications.MobileNetV2(input_shape=[128, 128, CHANNEL], include_top=False)\n",
        "base_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lQlzQh2SvLY"
      },
      "source": [
        "# U-Net에서 특징 추출 레이어로 사용할 계층들입니다.\n",
        "#이 층들의 활성화를 이용합시다\n",
        "layer_names = [\n",
        "    'block_1_expand_relu',   # 64x64\n",
        "    'block_3_expand_relu',   # 32x32\n",
        "    'block_6_expand_relu',   # 16x16\n",
        "    'block_13_expand_relu',  # 8x8\n",
        "    'block_16_project',      # 4x4\n",
        "]\n",
        "\n",
        "layers = [base_model.get_layer(name).output for name in layer_names]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ur5b805sSvLY"
      },
      "source": [
        "# U-Net은 다음과 같은 구조로, 일부는 정확한 지역화(Localization)을 수행하게 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6n6f_Bu8SvLZ"
      },
      "source": [
        "# U-net은 기본적으로 아래층으로 심층 특징 추출하는 층과, skip하는 층이 합쳐지는 구조# 특징추출 모델을 만듭시다.\n",
        "# 추가적인 설명은 다음 블로그에서 쉽게 이해할 수 있습니다.\n",
        "# https://medium.com/@msmapark2/u-net-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-u-net-convolutional-networks-for-biomedical-image-segmentation-456d6901b28a\n",
        "# 이를 'down_stack'한다고 합니다.\n",
        "down_stack = tf.keras.Model(inputs=base_model.input, outputs=layers)\n",
        "\n",
        "# 이미 특징 추출은 MobileNet에서 수행되었기 때문에, trainable = False\n",
        "down_stack.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KR9hyYfdSvLZ"
      },
      "source": [
        "# up_stack을 1회 수행하는 하나의 계층을 만들도록 upsample 함수를 정의합니다.\n",
        "def upsample(filters, size, apply_dropout=False):\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  result = tf.keras.Sequential()\n",
        "  result.add(\n",
        "    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,padding='same',kernel_initializer=initializer,use_bias=False))\n",
        "\n",
        "  result.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "  if apply_dropout:\n",
        "      result.add(tf.keras.layers.Dropout(0.5))\n",
        "\n",
        "  result.add(tf.keras.layers.ReLU())\n",
        "\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHFWUIczSvLZ"
      },
      "source": [
        "up_stack = [\n",
        "    upsample(512, 3),  # 4x4 -> 8x8\n",
        "    upsample(256, 3),  # 8x8 -> 16x16\n",
        "    upsample(128, 3),  # 16x16 -> 32x32\n",
        "    upsample(64, 3),   # 32x32 -> 64x64\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49RAdo1BSvLZ"
      },
      "source": [
        "# 모델을 만드는 함수를 정의합니다.\n",
        "def build_model(num_output_channels):\n",
        "    input_layer = tf.keras.layers.Input(shape=[128,128,3])\n",
        "    x = input_layer\n",
        "    \n",
        "#     모델을 다운 스택\n",
        "    skips = down_stack(x)\n",
        "    x = skips[-1]\n",
        "    skips = reversed(skips[:-1])\n",
        "    \n",
        "#     skip connection을 upsampling한다\n",
        "    for up, skip in zip(up_stack,skips):\n",
        "        x = up(x)\n",
        "        #  skip해서 넘어오는 connection과 down_stack에서 올라오는 up을 concatenate한다.\n",
        "        concat = tf.keras.layers.Concatenate()\n",
        "        x = concat([x,skip])\n",
        "        \n",
        "    # 현재 최종 계층의 output shape = (None, 64,64,1)\n",
        "    # 마지막 계층으로 Conv2DTranspose를 함으로써, output shape를 (None, 64, 64, Channel)로 지정한다\n",
        "    last_layer = tf.keras.layers.Conv2DTranspose(num_output_channels, 3, strides=2, padding='same') # 64x64 -> 128,128\n",
        "    \n",
        "    x = last_layer(x)\n",
        "    \n",
        "    return tf.keras.Model(inputs=input_layer, outputs=x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvPEgLvxSvLZ"
      },
      "source": [
        "# 모델을 컴파일합니다.\n",
        "### 각각의 픽셀에 대해서 {0,1,2} 3채널의 Sparse Categorical Cross Entropy를 수행하게 됩니다.\n",
        "### pixcel-wise (픽셀 단위의) 해석이기 때문에 Segmentation의 수행이 가능합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEjiDND_SvLa"
      },
      "source": [
        "OUTPUT_CHANNELS = 3\n",
        "model = build_model(OUTPUT_CHANNELS)\n",
        "\n",
        "model.compile(optimizer='adam',loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cK3GbVOSvLa"
      },
      "source": [
        "if colab:\n",
        "    from tensorflow.keras.utils import plot_model\n",
        "    plot_model(model, show_shapes=True)\n",
        "    # model.png 파일이 저장된 것을 확인할 수 있습니다.\n",
        "    plt.figure(figsize=(12,25))\n",
        "    plt.imshow(np.array(Image.open('model.png')))\n",
        "else:\n",
        "     model.summary()   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BakIPYppSvLa"
      },
      "source": [
        "# 초기 Prediction을 출력해 봅시다(epoch=0)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJI68d3xSvLa"
      },
      "source": [
        "sample_image, sample_mask = next(iter(dataset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpMlZMUpSvLa"
      },
      "source": [
        "def show_predictions(dataset=None, num=1,epoch=None):\n",
        "  if dataset:\n",
        "    for image, mask in dataset.take(num):\n",
        "      predicted_mask = model.predict(image)\n",
        "      # output 3채널 중에서 가장 큰 값들을 찾아서 1채널로 축소\n",
        "      predicted_mask = tf.argmax(predicted_mask, axis=-1)\n",
        "      predicted_mask = np.array(predicted_mask).reshape((10,128,128,1))\n",
        "#       display([image[0], mask[0], predicted_mask])\n",
        "      \n",
        "      plt.figure(figsize=(15,5))\n",
        "      for i in range(BATCH_SIZE):\n",
        "        plt.subplot(3,BATCH_SIZE,i+1)\n",
        "        plt.imshow(image[i])\n",
        "        plt.subplot(3,BATCH_SIZE,i+BATCH_SIZE+1)\n",
        "        plt.imshow(np.array(mask[i]).reshape(128,128))\n",
        "        plt.subplot(3,BATCH_SIZE,i+2 * BATCH_SIZE+1)\n",
        "        plt.imshow(predicted_mask[i].reshape(128,128))\n",
        "  else:\n",
        "    predicted_mask = model.predict(sample_image)\n",
        "    predicted_mask = tf.argmax(predicted_mask, axis=-1)\n",
        "    predicted_mask = np.array(predicted_mask).reshape((10,128,128,1))\n",
        "    plt.figure(figsize=(15,5))\n",
        "    if epoch:\n",
        "      plt.title(\"Current epoch :{}\".format(epoch))\n",
        "    for i in range(BATCH_SIZE):\n",
        "      plt.subplot(3,BATCH_SIZE,i+1)\n",
        "      plt.imshow(sample_image[i])\n",
        "      plt.subplot(3,BATCH_SIZE,i+BATCH_SIZE+1)\n",
        "      plt.imshow(np.array(sample_mask[i]).reshape(128,128))\n",
        "      plt.subplot(3,BATCH_SIZE,i+2 * BATCH_SIZE+1)\n",
        "      plt.imshow(predicted_mask[i].reshape(128,128))\n",
        "#     plt.show()\n",
        "    if epoch:\n",
        "        if colab:\n",
        "          save_path = \"/content/Surface_Crack_Segmentation/fig_saves/\"\n",
        "        else:\n",
        "          save_path = \"D:/_김정원/ss_class(AI)/Surface_Crack_Segmentation/fig_saves/\"  #  이미지 저장 경로를 변경해주세요\n",
        "        file_name = \"{}.png\".format(epoch)\n",
        "        plt.savefig(save_path+file_name)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a17mLXwTSvLb"
      },
      "source": [
        "# 트레이닝 되지않은 초기 데이터를 plot\n",
        "# 다소 약한 Mask들이 나오는 것을 볼 수 있다.\n",
        "show_predictions(dataset,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALDZY6xpSvLb"
      },
      "source": [
        "# 트레이닝"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pG_D23yISvLb"
      },
      "source": [
        "# 각 트레인 epoch가 끝날 때마다 트레이닝 sample_image, sample_mask로부터 학습과정을 시각화합니다.\n",
        "# tensorflow.keras.callbacks.Callback 클래스로부터 오버로딩합니다.\n",
        "from IPython.display import clear_output\n",
        "class DisplayCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    clear_output(wait=True)\n",
        "    show_predictions(epoch = epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlRETNVVSvLb"
      },
      "source": [
        "EPOCHS = 100\n",
        "\n",
        "STEPS_PER_EPOCH = train_dataset_size\n",
        "\n",
        "# 만일의 사태에 대비해서 체크포인트 콜백을 생성하고 적용하겠습니다.\n",
        "if colab:\n",
        "    save_dir = './ckpt_dat/'\n",
        "    checkpoint_path = save_dir + \"{epoch:03d}.ckpt\"\n",
        "    callback_autosave=tf.keras.callbacks.ModelCheckpoint(checkpoint_path,verbose=1,save_weights_only=True,save_freq=10)\n",
        "else:\n",
        "    checkpoint_path = \"D:/_김정원/ss_class(AI)/Surface_Crack_Segmentation/CKPT/{epoch:03d}.ckpt\"\n",
        "    callback_autosave=tf.keras.callbacks.ModelCheckpoint(checkpoint_path,verbose=1,save_weights_only=True,save_freq=10)\n",
        "\n",
        "\n",
        "# 이제 모델을 피팅합니다. 각 epoch이 끝날때마다 학습과정을 시각화합니다.\n",
        "model_history = model.fit(train_dataset, validation_data=test_dataset, \n",
        "                          epochs=EPOCHS, steps_per_epoch=STEPS_PER_EPOCH,\n",
        "                         callbacks=[DisplayCallback(),callback_autosave])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0Yfimi4SvLc"
      },
      "source": [
        "# 모델을 save합니다.\n",
        "# 사실 이미 체크포인트를 지정했기 때문에, 필요가 없습니다.\n",
        "if colab:\n",
        "  model.save(\"/content/Surface_Crack_Segmentation/MY_MODEL\")\n",
        "else:\n",
        "    model.save(\"D:/_김정원/ss_class(AI)/Surface_Crack_Segmentation/MY_MODEL\")\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0tmGti2SvLc"
      },
      "source": [
        "plt.figure(figsize=(12,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(model_history.history['accuracy'],label = 'train')\n",
        "plt.plot(model_history.history['val_accuracy'], label = 'validation')\n",
        "plt.title('accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(model_history.history['loss'],label='train')\n",
        "plt.plot(model_history.history['val_loss'],label='validation')\n",
        "plt.title('loss')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPGtZ3P9SvLc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}